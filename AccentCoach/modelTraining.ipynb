{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cbb5932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dccf15d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\natal\\anaconda3\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\natal\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\natal\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\natal\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\natal\\anaconda3\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59680637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: scikit-learn\n",
      "Version: 1.7.0\n",
      "Summary: A set of python modules for machine learning and data mining\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: BSD 3-Clause License\n",
      "\n",
      " Copyright (c) 2007-2024 The scikit-learn developers.\n",
      " All rights reserved.\n",
      "\n",
      " Redistribution and use in source and binary forms, with or without\n",
      " modification, are permitted provided that the following conditions are met:\n",
      "\n",
      " * Redistributions of source code must retain the above copyright notice, this\n",
      "   list of conditions and the following disclaimer.\n",
      "\n",
      " * Redistributions in binary form must reproduce the above copyright notice,\n",
      "   this list of conditions and the following disclaimer in the documentation\n",
      "   and/or other materials provided with the distribution.\n",
      "\n",
      " * Neither the name of the copyright holder nor the names of its\n",
      "   contributors may be used to endorse or promote products derived from\n",
      "   this software without specific prior written permission.\n",
      "\n",
      " THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
      " AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
      " IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
      " DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
      " FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
      " DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
      " SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
      " CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
      " OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      " OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "\n",
      " ----\n",
      "\n",
      " This binary distribution of scikit-learn also bundles the following software:\n",
      "\n",
      " ----\n",
      "\n",
      " Name: Microsoft Visual C++ Runtime Files\n",
      " Files: sklearn\\.libs\\*.dll\n",
      " Availability: https://learn.microsoft.com/en-us/visualstudio/releases/2015/2015-redistribution-vs\n",
      "\n",
      " Subject to the License Terms for the software, you may copy and distribute with your\n",
      " program any of the files within the followng folder and its subfolders except as noted\n",
      " below. You may not modify these files.\n",
      "\n",
      " C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\redist\n",
      "\n",
      " You may not distribute the contents of the following folders:\n",
      "\n",
      " C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\redist\\debug_nonredist\n",
      " C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\redist\\onecore\\debug_nonredist\n",
      "\n",
      " Subject to the License Terms for the software, you may copy and distribute the following\n",
      " files with your program in your program’s application local folder or by deploying them\n",
      " into the Global Assembly Cache (GAC):\n",
      "\n",
      " VC\\atlmfc\\lib\\mfcmifc80.dll\n",
      " VC\\atlmfc\\lib\\amd64\\mfcmifc80.dll\n",
      "\n",
      "Location: C:\\Users\\natal\\anaconda3\\Lib\\site-packages\n",
      "Requires: joblib, numpy, scipy, threadpoolctl\n",
      "Required-by: daal4py, imbalanced-learn, librosa, scikit-learn-intelex\n"
     ]
    }
   ],
   "source": [
    "!pip show scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7059c693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea2dfaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydub\n",
      "  Obtaining dependency information for pydub from https://files.pythonhosted.org/packages/a6/53/d78dc063216e62fc55f6b2eebb447f6a4b0a59f55c8406376f76bf959b08/pydub-0.25.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub\n",
      "Successfully installed pydub-0.25.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ec97d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planned natives: 160 (from 2 speakers)\n",
      "Planned non-natives: 160 (from 24 speakers)\n",
      "\n",
      "Extracting native features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2e061934ee4f8ea30e0b25ab2d15cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Natives:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting non-native features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac890bac6004db98b5cc92d97df2395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Non-Natives:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 320 items in 157.2s\n",
      "Counts -> Native(0): 160  Non-Native(1): 160\n",
      "Unique speakers: 26\n",
      "\n",
      "Fold A class counts (train / val): [80, 78] [80, 82]\n",
      "Fold B class counts (train / val): [80, 82] [80, 78]\n",
      "\n",
      "Fitting GridSearchCV...\n",
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n",
      "\n",
      "Best params: {'clf__C': 4}\n",
      "Best CV macro-F1 (2-fold): 0.9715595047527616\n",
      "\n",
      "=== Validation on Fold A (val speakers: ['ASI', 'BWC', 'ERMS', 'HJK', 'HQTV', 'LXC', 'MBMPS', 'NCC', 'PNV', 'SVBI', 'YBAA', 'YKWK', 'bdl'] ) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Native       1.00      1.00      1.00        80\n",
      "  Non-Native       1.00      1.00      1.00        82\n",
      "\n",
      "    accuracy                           1.00       162\n",
      "   macro avg       1.00      1.00      1.00       162\n",
      "weighted avg       1.00      1.00      1.00       162\n",
      "\n",
      "Confusion matrix:\n",
      " [[80  0]\n",
      " [ 0 82]]\n",
      "\n",
      "=== Validation on Fold B (val speakers: ['ABA', 'EBVS', 'HKK', 'NJS', 'RRBI', 'SKA', 'THV', 'TLV', 'TNI', 'TXHC', 'YDCK', 'ZHAA', 'slt'] ) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Native       1.00      1.00      1.00        80\n",
      "  Non-Native       1.00      1.00      1.00        78\n",
      "\n",
      "    accuracy                           1.00       158\n",
      "   macro avg       1.00      1.00      1.00       158\n",
      "weighted avg       1.00      1.00      1.00       158\n",
      "\n",
      "Confusion matrix:\n",
      " [[80  0]\n",
      " [ 0 78]]\n",
      "\n",
      "Refitting final model on ALL data...\n",
      "\n",
      "Saved model to: C:\\Users\\natal\\AccentCoach\\server\\model.joblib\n"
     ]
    }
   ],
   "source": [
    "import os, random, time, io, joblib, warnings\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import librosa\n",
    "from pydub import AudioSegment  \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# CONFIG — \n",
    "RANDOM_SEED = 42\n",
    "\n",
    "\n",
    "NATIVE_DIRS = [\n",
    "    \"cmu_us_bdl_arctic/wav\", \n",
    "    \"cmu_us_slt_arctic/wav\",  \n",
    "]\n",
    "NONNATIVE_ROOT = \"nonnative\"  \n",
    "\n",
    "\n",
    "NATIVE_FILES_PER_SPK   = 80   \n",
    "NONNATIVE_FILES_PER_SPK= 10  \n",
    "\n",
    "SR_TARGET = 16000\n",
    "N_MFCC    = 20\n",
    "\n",
    "PARAM_GRID = {\n",
    "    \"clf__C\": [0.25, 0.5, 1, 2, 4],\n",
    "}\n",
    "\n",
    "\n",
    "MODEL_OUT = r\"C:\\Users\\natal\\AccentCoach\\server\\model.joblib\" \n",
    "\n",
    "USE_AUG = False\n",
    "\n",
    "# Utils\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "def list_wavs(dir_path, limit=None):\n",
    "    if not os.path.isdir(dir_path):\n",
    "        return []\n",
    "    files = [os.path.join(dir_path, f) for f in os.listdir(dir_path)\n",
    "             if f.lower().endswith(\".wav\")]\n",
    "    files.sort()\n",
    "    return files[:limit] if limit else files\n",
    "\n",
    "def opus_roundtrip_load(wav_path, sr_target=SR_TARGET):\n",
    "\n",
    "    a = AudioSegment.from_file(wav_path)\n",
    "    a = a.set_channels(1).set_frame_rate(sr_target)\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    a.export(buf, format=\"webm\", codec=\"libopus\", bitrate=\"32k\")  # ~browser-like\n",
    "    buf.seek(0)\n",
    "\n",
    "    b = AudioSegment.from_file(buf, format=\"webm\")\n",
    "    b = b.set_channels(1).set_frame_rate(sr_target)\n",
    "\n",
    "    arr = np.array(b.get_array_of_samples()).astype(np.float32)\n",
    "   \n",
    "    if b.sample_width == 2:\n",
    "        arr /= 32768.0\n",
    "    elif b.sample_width == 4:\n",
    "        arr /= 2147483648.0\n",
    "    # Ensure [-1, 1]\n",
    "    arr = np.clip(arr, -1.0, 1.0)\n",
    "    return arr, sr_target\n",
    "\n",
    "def light_aug(y, sr):\n",
    "   \n",
    "    gain = 10 ** (np.random.uniform(-0.6, 0.6) / 20)\n",
    "    y = np.clip(y * gain, -1.0, 1.0)\n",
    "    rate = np.random.uniform(0.95, 1.05)\n",
    "    try:\n",
    "        y = librosa.effects.time_stretch(y, rate)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if np.random.rand() < 0.5:\n",
    "        noise = np.random.randn(len(y)).astype(np.float32) * 0.004\n",
    "        y = np.clip(y + noise, -1.0, 1.0)\n",
    "    return y\n",
    "\n",
    "def extract_features_320_from_array(y, sr, n_mfcc=N_MFCC):\n",
    "    \n",
    "    y, _ = librosa.effects.trim(y, top_db=30)\n",
    "\n",
    "    min_len = int(0.5 * SR_TARGET)\n",
    "    if len(y) < min_len:\n",
    "        y = np.pad(y, (0, min_len - len(y)))\n",
    "\n",
    "    peak = np.max(np.abs(y)) if y.size else 0.0\n",
    "    if peak > 0:\n",
    "        y = y / peak\n",
    "\n",
    "    #MFCCs\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    d1   = librosa.feature.delta(mfcc)\n",
    "    d2   = librosa.feature.delta(mfcc, order=2)\n",
    "\n",
    "    #spectral stats\n",
    "    zcr  = librosa.feature.zero_crossing_rate(y)[0]\n",
    "    cent = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "    bw   = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]\n",
    "    roll = librosa.feature.spectral_rolloff(y=y, sr=sr, roll_percent=0.95)[0]\n",
    "\n",
    "    def stats(v):\n",
    "        return np.array([np.mean(v), np.std(v), np.median(v),\n",
    "                         np.percentile(v, 5), np.percentile(v, 95)],\n",
    "                        dtype=np.float32)\n",
    "    def agg(mat):\n",
    "        return np.concatenate([stats(row) for row in mat], axis=0)\n",
    "\n",
    "    feat = np.concatenate([\n",
    "        agg(mfcc),  # 20 * 5 = 100\n",
    "        agg(d1),    # 100\n",
    "        agg(d2),    # 100\n",
    "        stats(zcr), # 5\n",
    "        stats(cent),# 5\n",
    "        stats(bw),  # 5\n",
    "        stats(roll) # 5  -> 320 total\n",
    "    ], axis=0).astype(np.float32)\n",
    "    feat[~np.isfinite(feat)] = 0.0\n",
    "    return feat\n",
    "\n",
    "def extract_features(path):\n",
    "    \"\"\"\n",
    "    Full domain-matched loader + feature pipeline.\n",
    "    \"\"\"\n",
    "    y, sr = opus_roundtrip_load(path, SR_TARGET)\n",
    "    if USE_AUG:\n",
    "        y = light_aug(y, sr)\n",
    "    return extract_features_320_from_array(y, sr)\n",
    "\n",
    "# Build balanced file lists\n",
    "native_items = []  \n",
    "for d, spk in [(NATIVE_DIRS[0], \"bdl\"), (NATIVE_DIRS[1], \"slt\")]:\n",
    "    files = list_wavs(d, limit=NATIVE_FILES_PER_SPK)\n",
    "    native_items.extend([(p, spk) for p in files])\n",
    "\n",
    "native_total = len(native_items)\n",
    "if native_total == 0:\n",
    "    raise RuntimeError(\"No native files found. Check NATIVE_DIRS paths.\")\n",
    "\n",
    "# Non-natives \n",
    "nonnative_by_spk = {}  \n",
    "if os.path.isdir(NONNATIVE_ROOT):\n",
    "    for spk in sorted(os.listdir(NONNATIVE_ROOT)):\n",
    "        wav_dir = os.path.join(NONNATIVE_ROOT, spk, \"wav\")\n",
    "        if os.path.isdir(wav_dir):\n",
    "            files = list_wavs(wav_dir, limit=NONNATIVE_FILES_PER_SPK)\n",
    "            if files:\n",
    "                random.shuffle(files)\n",
    "                nonnative_by_spk[spk] = files\n",
    "\n",
    "if not nonnative_by_spk:\n",
    "    raise RuntimeError(\"No non-native files found. Check NONNATIVE_ROOT structure.\")\n",
    "\n",
    "def round_robin_select(spk2files, target):\n",
    "    spks = list(spk2files.keys())\n",
    "    idxs = {s: 0 for s in spks}\n",
    "    picked = []\n",
    "    while len(picked) < target:\n",
    "        any_added = False\n",
    "        for s in spks:\n",
    "            i = idxs[s]\n",
    "            if i < len(spk2files[s]):\n",
    "                picked.append((spk2files[s][i], s))\n",
    "                idxs[s] += 1\n",
    "                any_added = True\n",
    "                if len(picked) >= target:\n",
    "                    break\n",
    "        if not any_added:\n",
    "            break\n",
    "    return picked\n",
    "\n",
    "selected_nonnat = round_robin_select(nonnative_by_spk, native_total)\n",
    "print(f\"Planned natives: {native_total} (from 2 speakers)\")\n",
    "print(f\"Planned non-natives: {len(selected_nonnat)} (from {len(nonnative_by_spk)} speakers)\")\n",
    "if len(selected_nonnat) < native_total:\n",
    "    print(f\" Not enough non-native files to match natives (short by {native_total - len(selected_nonnat)})\")\n",
    "\n",
    "# Extract features (with tqdm)\n",
    "X, y, groups, paths = [], [], [], []\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "print(\"\\nExtracting native features...\")\n",
    "for p, spk in tqdm(native_items, desc=\"Natives\", dynamic_ncols=True):\n",
    "    try:\n",
    "        X.append(extract_features(p)); y.append(0); groups.append(spk); paths.append(p)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Native file failed: {p} -> {e}\")\n",
    "\n",
    "print(\"Extracting non-native features...\")\n",
    "for p, spk in tqdm(selected_nonnat, desc=\"Non-Natives\", dynamic_ncols=True):\n",
    "    try:\n",
    "        X.append(extract_features(p)); y.append(1); groups.append(spk); paths.append(p)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Non-native file failed: {p} -> {e}\")\n",
    "\n",
    "X = np.vstack(X).astype(np.float32)\n",
    "y = np.asarray(y, dtype=np.int64)\n",
    "groups = np.asarray(groups)\n",
    "\n",
    "print(f\"\\nLoaded {len(y)} items in {time.perf_counter()-t0:.1f}s\")\n",
    "print(\"Counts -> Native(0):\", int((y==0).sum()), \" Non-Native(1):\", int((y==1).sum()))\n",
    "print(\"Unique speakers:\", len(np.unique(groups)))\n",
    "\n",
    "\n",
    "# Manual 2-fold speaker-disjoint CV\n",
    "# Fold A validates on native 'bdl' + half of non-native speakers\n",
    "# Fold B validates on native 'slt' + the other half\n",
    "native_spk = sorted(list(set([g for (g,l) in zip(groups,y) if l==0])))\n",
    "if len(native_spk) < 2:\n",
    "    raise RuntimeError(f\"Need at least 2 native speakers; found {len(native_spk)}\")\n",
    "\n",
    "nonnat_spk = sorted(list(set([g for (g,l) in zip(groups,y) if l==1])))\n",
    "random.shuffle(nonnat_spk)\n",
    "half = len(nonnat_spk) // 2\n",
    "halfA, halfB = set(nonnat_spk[:half]), set(nonnat_spk[half:])\n",
    "\n",
    "valA = set([native_spk[0]]) | halfA\n",
    "valB = set([native_spk[1]]) | halfB\n",
    "\n",
    "train_idx_A = np.where(~np.isin(groups, list(valA)))[0]; val_idx_A = np.where(np.isin(groups, list(valA)))[0]\n",
    "train_idx_B = np.where(~np.isin(groups, list(valB)))[0]; val_idx_B = np.where(np.isin(groups, list(valB)))[0]\n",
    "\n",
    "def has_both(idx):\n",
    "    return len(np.unique(y[idx])) == 2\n",
    "\n",
    "if not (has_both(train_idx_A) and has_both(val_idx_A) and has_both(train_idx_B) and has_both(val_idx_B)):\n",
    "    raise RuntimeError(\"A fold lost a class; adjust the per-speaker caps to ensure both classes in train/val.\")\n",
    "\n",
    "print(\"\\nFold A class counts (train / val):\",\n",
    "      np.bincount(y[train_idx_A], minlength=2).tolist(),\n",
    "      np.bincount(y[val_idx_A],   minlength=2).tolist())\n",
    "print(\"Fold B class counts (train / val):\",\n",
    "      np.bincount(y[train_idx_B], minlength=2).tolist(),\n",
    "      np.bincount(y[val_idx_B],   minlength=2).tolist())\n",
    "\n",
    "cv_folds = [(train_idx_A, val_idx_A), (train_idx_B, val_idx_B)]\n",
    "\n",
    "\n",
    "# Grid search (LogReg) with 2-fold CV\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        solver='lbfgs',\n",
    "        random_state=RANDOM_SEED\n",
    "    )),\n",
    "])\n",
    "\n",
    "print(\"\\nFitting GridSearchCV...\")\n",
    "grid = GridSearchCV(\n",
    "    pipe,\n",
    "    PARAM_GRID,\n",
    "    cv=cv_folds,\n",
    "    n_jobs=-1,\n",
    "    scoring='f1_macro',   # macro F1 to balance both classes\n",
    "    verbose=2,\n",
    "    error_score='raise'\n",
    ")\n",
    "\n",
    "grid.fit(X, y)\n",
    "best = grid.best_estimator_\n",
    "print(\"\\nBest params:\", grid.best_params_)\n",
    "print(\"Best CV macro-F1 (2-fold):\", grid.best_score_)\n",
    "\n",
    "# Validate on each fold with best\n",
    "print(\"\\n=== Validation on Fold A (val speakers:\", sorted(list(valA)), \") ===\")\n",
    "y_pred_A = best.predict(X[val_idx_A])\n",
    "print(classification_report(y[val_idx_A], y_pred_A, target_names=[\"Native\", \"Non-Native\"]))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y[val_idx_A], y_pred_A))\n",
    "\n",
    "print(\"\\n=== Validation on Fold B (val speakers:\", sorted(list(valB)), \") ===\")\n",
    "y_pred_B = best.predict(X[val_idx_B])\n",
    "print(classification_report(y[val_idx_B], y_pred_B, target_names=[\"Native\", \"Non-Native\"]))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y[val_idx_B], y_pred_B))\n",
    "\n",
    "# Refit final model on ALL data and save\n",
    "final = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        solver='lbfgs',\n",
    "        random_state=RANDOM_SEED,\n",
    "        C=best.get_params()['clf__C']\n",
    "    )),\n",
    "])\n",
    "\n",
    "print(\"\\nRefitting final model on ALL data...\")\n",
    "final.fit(X, y)\n",
    "\n",
    "os.makedirs(os.path.dirname(MODEL_OUT), exist_ok=True)\n",
    "joblib.dump(final, MODEL_OUT)\n",
    "print(f\"\\nSaved model to: {MODEL_OUT}\")\n",
    "\n",
    "def predict_file(path, model=final):\n",
    "    yx, sr = opus_roundtrip_load(path, SR_TARGET)\n",
    "    feat = extract_features_320_from_array(yx, sr).reshape(1, -1)\n",
    "    proba = model.predict_proba(feat)[0]\n",
    "    classes = list(model.named_steps['clf'].classes_)\n",
    "    p_native = float(proba[classes.index(0)])\n",
    "    p_non    = float(proba[classes.index(1)])\n",
    "    pred     = int(model.predict(feat)[0])\n",
    "    label    = \"Native\" if pred == 0 else \"Non-Native\"\n",
    "    print(os.path.basename(path), \"->\", label,\n",
    "          f\"(p_native={p_native:.2f}, p_non_native={p_non:.2f})\")\n",
    "    return pred, (p_native, p_non)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a415b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arctic_a0009.wav -> Native (p_native=1.00, p_non_native=0.00)\n",
      "arctic_a0003.wav -> Non-Native (p_native=0.00, p_non_native=1.00)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 7.675481947904457e-08, 0.9999999232451806)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib, numpy as np, os\n",
    "\n",
    "def extract_features(path, sr_target=16000, n_mfcc=20):\n",
    "    import librosa, numpy as np\n",
    "    y, sr = librosa.load(path, sr=sr_target, mono=True)\n",
    "    y, _ = librosa.effects.trim(y, top_db=30)\n",
    "    min_len = int(0.5 * sr_target)\n",
    "    if len(y) < min_len: y = np.pad(y, (0, min_len - len(y)))\n",
    "    peak = np.max(np.abs(y)) if y.size else 0.0\n",
    "    if peak > 0: y = y / peak\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr_target, n_mfcc=n_mfcc)\n",
    "    d1   = librosa.feature.delta(mfcc)\n",
    "    d2   = librosa.feature.delta(mfcc, order=2)\n",
    "    zcr  = librosa.feature.zero_crossing_rate(y)[0]\n",
    "    cent = librosa.feature.spectral_centroid(y=y, sr=sr_target)[0]\n",
    "    bw   = librosa.feature.spectral_bandwidth(y=y, sr=sr_target)[0]\n",
    "    roll = librosa.feature.spectral_rolloff(y=y, sr=sr_target, roll_percent=0.95)[0]\n",
    "\n",
    "    def stats(v):\n",
    "        return np.array([np.mean(v), np.std(v), np.median(v),\n",
    "                         np.percentile(v, 5), np.percentile(v, 95)], dtype=np.float32)\n",
    "    def agg(mat): return np.concatenate([stats(c) for c in mat], axis=0)\n",
    "\n",
    "    feat = np.concatenate([agg(mfcc), agg(d1), agg(d2), stats(zcr), stats(cent), stats(bw), stats(roll)], 0).astype(np.float32)\n",
    "    feat[~np.isfinite(feat)] = 0.0\n",
    "    return feat\n",
    "\n",
    "m = joblib.load(\"model.joblib\")\n",
    "\n",
    "def predict_file(path):\n",
    "    x = extract_features(path).reshape(1, -1)\n",
    "    proba = m.predict_proba(x)[0]\n",
    "    cls = list(m.named_steps['clf'].classes_) if 'clf' in getattr(m, 'named_steps', {}) else list(m.classes_)\n",
    "    p_native = float(proba[cls.index(0)]) if 0 in cls else None\n",
    "    p_non    = float(proba[cls.index(1)]) if 1 in cls else None\n",
    "    pred = int(m.predict(x)[0])\n",
    "    label = \"Native\" if pred == 0 else \"Non-Native\"\n",
    "    print(os.path.basename(path), \"->\", label, f\"(p_native={p_native:.2f}, p_non_native={p_non:.2f})\")\n",
    "    return pred, p_native, p_non\n",
    "\n",
    "\n",
    "predict_file(\"cmu_us_bdl_arctic/wav/arctic_a0009.wav\")     \n",
    "predict_file(\"nonnative/LXC/wav/arctic_a0003.wav\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3599bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
